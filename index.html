<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>Track-On: Transformer-based Online Point Tracking with Memory</title>
	<meta property="og:image" content="Path to my teaser.jpg"/>
	<meta property="og:title" content="SOLV" />
	<meta property="og:description" content="SOLV" />
    <meta property="twitter:card"          content="SOLV" />
    <meta property="twitter:title"         content="SOLV" />
    <meta property="twitter:description"   content="SOLV" />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
      <b>Track-On:</b> <br>
      Transformer-based Online Point Tracking with Memory
    </div>


    <br><br>
    <div class="author">
        <a href="https://gorkaydemir.github.io" target="_blank">Gorkay Aydemir</a><sup>1</sup>
    </div>
    <div class="author">
      <a href="">Xiongyi Cai</a><sup>3</sup>
  </div>
    <div class="author">
        <a href="https://weidixie.github.io" target="_blank">Weidi Xie</a><sup>3</sup>
    </div>
    <div class="author">
        <a href="https://mysite.ku.edu.tr/fguney/" target="_blank">Fatma Guney</a><sup>1, 2</sup>
    </div>

    <br><br>
    <div class="affiliation"><sup>1&nbsp;</sup><a href="https://cs.ku.edu.tr" target="_blank">Department of Computer Engineering, Koc University</a></div>
    <div class="affiliation"><sup>2&nbsp;</sup><a href="https://ai.ku.edu.tr" target="_blank">KUIS AI Center</a></div>
    <div class="affiliation"><sup>3&nbsp;</sup><a href="https://en.sjtu.edu.cn" target="_blank">School of Artificial Intelligence, Shanghai Jiao Tong University</a></div>

    
    <div class="venue">
      <b>ICLR 2025</b>
    </div>
    <br><br>


    <!-- <div class="links">Paper <a href="https://arxiv.org/abs/2307.14187" target="_blank"> [arXiv]</a></div> -->
    <div class="links">Paper <a href="https://arxiv.org/abs/2501.18487" target="_blank"> [arXiv]</a></div>
    <div class="links">Code <a href="https://github.com/gorkaydemir/track_on" target="_blank"> [GitHub]</a></div>
    <div class="links">Cite <a href="./resources/bibtex.txt" target="_blank"> [BibTeX]</a></div>

    <br>
    <br>
    <br>

    <div class="row">
      <div class="cropped">
        <img style="width: 82%;" src="./resources/0_frontman.gif"/>
      </div>
      <div class="cropped">
        <img style="width: 80%;" src="./resources/0_hyena.gif"/>
      </div>
    </div><br>

    <div class="row">
      <div class="cropped">
        <img style="width: 80%;" src="./resources/0_messi.gif"/>
      </div>
      <div class="cropped">
        <img style="width:80%;" src="./resources/0_drone_car.gif"/>
      </div>
    </div><br>


    <div class="box"><b> <FONT COLOR="RED">TL;DR</FONT></b> we introduce <b>Track-On</b>, a transformer-based model for <b>online long-term point tracking</b>, leveraging spatial and context memory to enable <b>frame-by-frame tracking</b> without access to future frames, achieving <b>state-of-the-art</b> performance among all online and offline approaches across multiple datasets. </div>

    <br>

    <br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
      In this paper, we consider the problem of long-term point tracking, which requires consistent identification of points across multiple frames in a video, despite changes in appearance, lighting, perspective, and occlusions. We target online tracking on a frame-by-frame basis, making it suitable for real-world, streaming scenarios. Specifically, we introduce <b>Track-On</b>, a simple transformer-based model designed for online long-term point tracking. 
      Unlike prior methods that depend on full temporal modeling, 
      our model processes video frames causally without access to future frames, leveraging two memory modules, spatial memory and context memory, to capture temporal information and maintain reliable point tracking over long time horizons. 
      At inference time, it employs patch classification and refinement to identify correspondences and track points with high accuracy. 
      Through extensive experiments, we demonstrate that <b>Track-On</b> sets a new state-of-the-art for online models and delivers superior or competitive results compared to offline approaches on seven datasets, including the TAP-Vid benchmark. Our method offers a robust and scalable solution for real-time tracking in diverse applications.
    </p>

    <br><br>
    <hr>

    <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method_overview.png"
         alt="Method overview figure"/>
    <br><br>
    <p style="width: 80%;">
      We propose <b>Track-On</b>, a transformer-based model for online long-term point tracking. Our approach processes frames sequentially without future access, leveraging memory modules for robust tracking. <br>
      
      <br><br><br>
      <h2>Tracking Pipeline</h2>
      <img style="width: 80%;" src="./resources/tracking_pipeline.png"
           alt="Tracking pipeline"/>
      <br><br>
      <p style="width: 80%; text-align: left;">
      (i) <b>Visual Encoder</b>: Extracts frame-wise features using a vision transformer backbone. <br>
      (ii) <b>Query Decoder</b>: Processes point queries with attention mechanisms to update features dynamically. <br>
      (iii) <b>Patch Classification & Refinement</b>: Instead of directly regressing coordinates, the model treats tracking as a classification task by selecting the most likely patch for a given query and refining the estimate with local offsets. <br>
      (iv) <b>Visibility Estimation</b>: Determines whether a point remains visible in the frame. <br>
      </p>
      <br>
      <h2>Memory Modules</h2>
      <img style="width: 80%;" src="./resources/memory_modules.png"
           alt="Memory modules"/>
      <br><br>
      <p style="width: 80%; text-align: left;">
      (i) <b>Spatial Memory</b>: Stores local regions around past predictions to mitigate feature drift. <br>
      (ii) <b>Context Memory</b>: Maintains a history of past query embeddings to preserve long-term tracking consistency. <br>
      (iii) <b>Memory Update & Retrieval</b>: Updates memory dynamically to refine predictions and track occluded points. <br>
      </p>

    </p>
    <br>

    <hr>

    <h1>Results</h1>

    <h2>Quantitative Results on TAP-Vid Benchmark</h2>
    <img style="width: 80%;" src="./resources/tapvid.png"
         alt="TAP-Vid comparison table"/>
    <br><br>

    <p style="width: 100%; text-align: left;">
      We compare Track-On against existing online and offline point tracking models on the TAP-Vid benchmark, evaluating performance across three datasets: 
      DAVIS, RGB-Stacking, and Kinetics. Offline models have access to full video sequences or temporal windows, leveraging bidirectional information for improved accuracy, while online models process frames sequentially, making them more suitable for real-time applications.
      Our model Track-On establishes a new state-of-the-art among online methods, outperforming Online TAPIR across all datasets. Despite operating in an online setting, Track-On achieves performance comparable to or exceeding several offline approaches. <br><br>
    
    </p>


    <h2>Results on RoboTAP, Dynamic Replica, and BADJA</h2>
    <img style="width: 80%;" src="./resources/robotap_dr_badja.png"
        alt=" RoboTAP, Dynamic Replica, and BADJA comparison table"/>
    <br><br>

    <p style="width: 100%; text-align: left;">
      We further evaluate Track-On on RoboTAP, Dynamic Replica, and BADJA, which test long-term tracking in real-world robotic and 3D motion scenarios. 
      Track-On not only outperforms Online TAPIR across all datasets but also surpasses several offline models, demonstrating strong temporal consistency and robustness to occlusions. <br><br>
    
    </p>

    <h1>Efficiency</h1>

    <img style="width: 50%;" src="./resources/efficiency.png"
         alt="Efficiency plot figure"/>
    <br><br>

    <p style="width: 100%; text-align: left;">
      The table above presents results across different memory sizes, demonstrating Track-On's flexibility in adapting to various computational constraints.
      Track-On is designed for efficiency, enabling real-time frame-by-frame tracking with minimal computational overhead. Unlike offline methods that require full video access and heavy temporal processing, our approach operates sequentially, significantly reducing memory and compute requirements.
      Our model runs at <b>over 15 FPS</b> while using <b>less than 1 GB of GPU memory</b>, making it highly efficient for real-time applications and consumer-grade hardware. <br><br>
    </p>
    



    <br>

    <hr>



    <h1>Paper</h1>

    <div class="paper-info"style="width: 80%;">
        <h3>Track-On: Transformer-based Online Point Tracking with Memory</h3>
        <p>Gorkay Aydemir, Xiongyi Cai, Weidi Xie and Fatma Guney</p>

        <p>ICLR 2025</p>
        <pre><code>@InProceedings{Aydemir2025ICLR,
          author = {Aydemir, G\"orkay and Cai, Xiongyi and Xie, Weidi and G\"uney, Fatma},
          title = {{Track-On}: Transformer-based Online Point Tracking with Memory},
          booktitle = {The Thirteenth International Conference on Learning Representations},
          year      = {2025}}
                </code></pre>
    </div>

    <br><br>

</div>

</body>

</html>
